/**
 * LanguageTool, a natural language style checker
 * Copyright (C) 2005 Daniel Naber (http://www.danielnaber.de)
 *
 * This library is free software; you can redistribute it and/or
 * modify it under the terms of the GNU Lesser General Public
 * License as published by the Free Software Foundation; either
 * version 2.1 of the License, or (at your option) any later version.
 *
 * This library is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
 * Lesser General Public License for more details.
 *
 * You should have received a copy of the GNU Lesser General Public
 * License along with this library; if not, write to the Free Software
 * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  02110-1301
 * USA
 */
package org.languagetool.tagging.disambiguation.uk;


import java.io.IOException;
import java.util.Arrays;
import java.util.List;
import java.util.Map;
import org.junit.Assert;
import org.junit.Test;
import org.languagetool.AnalyzedSentence;
import org.languagetool.AnalyzedTokenReadings;
import org.languagetool.JLanguageTool;
import org.languagetool.TestTools;
import org.languagetool.language.Ukrainian;
import org.languagetool.tagging.disambiguation.Disambiguator;
import org.languagetool.tagging.disambiguation.rules.DisambiguationRuleTest;
import org.languagetool.tagging.disambiguation.uk.SimpleDisambiguator.TokenMatcher;
import org.languagetool.tagging.disambiguation.xx.DemoDisambiguator;
import org.languagetool.tagging.uk.UkrainianTagger;
import org.languagetool.tokenizers.SRXSentenceTokenizer;
import org.languagetool.tokenizers.uk.UkrainianWordTokenizer;


public class UkrainianDisambiguationRuleTest extends DisambiguationRuleTest {
    private UkrainianTagger tagger;

    private UkrainianWordTokenizer tokenizer;

    private SRXSentenceTokenizer sentenceTokenizer;

    private UkrainianHybridDisambiguator disambiguator;

    private DemoDisambiguator demoDisambiguator;

    private Disambiguator chunker;

    @Test
    public void testDisambiguator() throws IOException {
        TestTools.myAssert("????????? ?? ?????", ("/[null]SENT_START ?????????/[?????????]verb:imperf:inf  /[null]null ??/[?? ?????]<adv>|??/[??]prep  /[null]null " + "?????/[??????]verb:perf:futr:s:1:xp2|?????/[?? ?????]<adv>"), tokenizer, sentenceTokenizer, tagger, disambiguator);
        TestTools.myAssert("??????? ???? ??????.", "/[null]SENT_START ???????/[??????]verb:perf:past:f|???????/[????????]adj:f:v_kly|???????/[????????]adj:f:v_naz  /[null]null ????/[????]noun:anim:f:v_naz:prop:fname|????/[?????]adj:f:v_kly:compb|????/[?????]adj:f:v_naz:compb  /[null]null ??????/[??????]adv ./[null]null", tokenizer, sentenceTokenizer, tagger, demoDisambiguator);
        TestTools.myAssert("??????? ???? ??????.", "/[null]SENT_START ???????/[??????]verb:perf:past:f  /[null]null ????/[????]noun:anim:f:v_naz:prop:fname  /[null]null ??????/[??????]adv ./[null]null", tokenizer, sentenceTokenizer, tagger, disambiguator);
    }

    @Test
    public void testDisambiguatorForInanimVKly() throws IOException {
        TestTools.myAssert("???????? ?????", ("/[null]SENT_START ????????/[?????????]adj:n:v_kly:&adjp:pasv:perf:coll|????????/[?????????]adj:n:v_naz:&adjp:pasv:perf:coll|????????/[?????????]adj:n:v_zna:&adjp:pasv:perf:coll" + "  /[null]null ?????/[?????]noun:inanim:n:v_naz|?????/[?????]noun:inanim:n:v_zna|?????/[?????]verb:imperf:past:n"), tokenizer, sentenceTokenizer, tagger, disambiguator);
        TestTools.myAssert("??", ("/[null]SENT_START ??/[??]noun:inanim:n:v_dav:nv|??/[??]noun:inanim:n:v_mis:nv|??/[??]noun:inanim:n:v_naz:nv|??/[??]noun:inanim:n:v_oru:nv|??/[??]noun:inanim:n:v_rod:nv" + "|??/[??]noun:inanim:n:v_zna:nv|??/[??]noun:inanim:p:v_dav:nv|??/[??]noun:inanim:p:v_mis:nv|??/[??]noun:inanim:p:v_naz:nv|??/[??]noun:inanim:p:v_oru:nv|??/[??]noun:inanim:p:v_rod:nv|??/[??]noun:inanim:p:v_zna:nv|??/[??]prep"), tokenizer, sentenceTokenizer, tagger, disambiguator);
        TestTools.myAssert("??? ??????????? ????...", ("/[null]SENT_START ???/[???]adj:m:v_kly:&pron:pos|???/[???]adj:m:v_naz:&pron:pos|???/[???]adj:m:v_zna:rinanim:&pron:pos  /[null]null" + (" ???????????/[???????????]adj:m:v_kly|???????????/[???????????]adj:m:v_naz|???????????/[???????????]adj:m:v_zna:rinanim  /[null]null" + " ????/[????]noun:inanim:m:v_dav|????/[????]noun:inanim:m:v_kly|????/[????]noun:inanim:m:v_mis|????/[????]noun:inanim:m:v_rod .../[null]null")), tokenizer, sentenceTokenizer, tagger, disambiguator);
        // still v_kly
        TestTools.myAssert("????? ??????!", ("/[null]SENT_START ?????/[?????]adj:m:v_kly:compb|?????/[?????]adj:m:v_naz:compb|?????/[?????]adj:m:v_zna:rinanim:compb" + "  /[null]null ??????/[??????]noun:inanim:m:v_dav|??????/[??????]noun:inanim:m:v_kly|??????/[??????]noun:inanim:m:v_mis !/[null]null"), tokenizer, sentenceTokenizer, tagger, disambiguator);
    }

    @Test
    public void testDisambiguatorForPluralNames() throws IOException {
        // + "|???????/[???????]noun:anim:m:v_naz:prop:lname:xp1|???????/[???????]noun:inanim:m:v_naz:prop:xp2|???????/[???????]noun:inanim:m:v_zna:prop:xp2",
        // + "|???????/[???????]noun:anim:f:v_dav:nv:np:prop:lname|???????/[???????]noun:anim:f:v_kly:nv:np:prop:lname|???????/[???????]noun:anim:f:v_mis:nv:np:prop:lname|???????/[???????]noun:anim:f:v_naz:nv:np:prop:lname|???????/[???????]noun:anim:f:v_oru:nv:np:prop:lname|???????/[???????]noun:anim:f:v_rod:nv:np:prop:lname|???????/[???????]noun:anim:f:v_zna:nv:np:prop:lname|???????/[???????]noun:anim:f:v_dav:nv:np:prop:lname|???????/[???????]noun:anim:f:v_kly:nv:np:prop:lname|???????/[???????]noun:anim:f:v_mis:nv:np:prop:lname|???????/[???????]noun:anim:f:v_naz:nv:np:prop:lname|???????/[???????]noun:anim:f:v_oru:nv:np:prop:lname|???????/[???????]noun:anim:f:v_rod:nv:np:prop:lname|???????/[???????]noun:anim:f:v_zna:nv:np:prop:lname|???????/[???????]noun:anim:m:v_naz:prop:lname:xp1|???????/[???????]noun:inanim:m:v_naz:prop:xp2|???????/[???????]noun:inanim:m:v_zna:prop:xp2",
        // + "  /[null]null ???????/[??????]noun:anim:p:v_rod:prop:fname|???????/[??????]noun:anim:p:v_zna:prop:fname|???????/[???????]adj:m:v_kly|???????/[???????]adj:m:v_naz|???????/[???????]adj:m:v_zna:rinanim"
        // + "|???????/[???????]noun:anim:f:v_dav:nv:np:prop:lname|???????/[???????]noun:anim:f:v_kly:nv:np:prop:lname|???????/[???????]noun:anim:f:v_mis:nv:np:prop:lname|???????/[???????]noun:anim:f:v_naz:nv:np:prop:lname|???????/[???????]noun:anim:f:v_oru:nv:np:prop:lname|???????/[???????]noun:anim:f:v_rod:nv:np:prop:lname|???????/[???????]noun:anim:f:v_zna:nv:np:prop:lname"
        // + "|???????/[???????]noun:anim:m:v_naz:prop:lname:xp1|???????/[???????]noun:anim:[m:v_naz:prop:lname:xp1|???????/[???????]noun:inanim:m:v_naz:prop:xp2|???????/[???????]noun:inanim:m:v_zna:prop:xp2",
        TestTools.myAssert("???????? ???????", ("/[null]SENT_START ????????/[????????]adj:p:v_mis:&pron:gen|????????/[????????]adj:p:v_rod:&pron:gen|????????/[????????]adj:p:v_zna:ranim:&pron:gen" + ((("  /[null]null ???????/[??????]noun:anim:p:v_rod:prop:fname|???????/[??????]noun:anim:p:v_zna:prop:fname|???????/[???????]adj:m:v_kly|???????/[???????]adj:m:v_naz|???????/[???????]adj:m:v_zna:rinanim" + "|???????/[???????]noun:anim:f:v_dav:nv:np:prop:lname|???????/[???????]noun:anim:f:v_kly:nv:np:prop:lname|???????/[???????]noun:anim:f:v_mis:nv:np:prop:lname|???????/[???????]noun:anim:f:v_naz:nv:np:prop:lname|???????/[???????]noun:anim:f:v_oru:nv:np:prop:lname|???????/[???????]noun:anim:f:v_rod:nv:np:prop:lname|???????/[???????]noun:anim:f:v_zna:nv:np:prop:lname") + "|???????/[???????]noun:anim:m:v_naz:prop:lname:xp1") + "|???????/[???????]noun:inanim:m:v_naz:prop:geo:xp2|???????/[???????]noun:inanim:m:v_zna:prop:geo:xp2")), tokenizer, sentenceTokenizer, tagger, disambiguator);
        TestTools.myAssert("2 ??????", ("/[null]SENT_START 2/[2]number" + "  /[null]null ??????/[??????]noun:anim:m:v_mis:prop:fname|??????/[??????]noun:anim:p:v_kly:prop:fname|??????/[??????]noun:anim:p:v_naz:prop:fname"), tokenizer, sentenceTokenizer, tagger, disambiguator);
        TestTools.myAssert("????? ????????", ("/[null]SENT_START ?????/[????]noun:anim:p:v_rod:prop:fname|?????/[????]noun:anim:p:v_zna:prop:fname|?????/[?????]adj:m:v_kly|?????/[?????]adj:m:v_naz|?????/[?????]adj:m:v_zna:rinanim" + "  /[null]null ????????/[???????]noun:inanim:p:v_rod:prop:geo:ns|????????/[???????]noun:anim:p:v_rod:prop:lname|????????/[???????]noun:anim:p:v_zna:prop:lname"), tokenizer, sentenceTokenizer, tagger, disambiguator);
        TestTools.myAssert("???????? ???????", ("/[null]SENT_START ????????/[????????]adj:p:v_mis:&pron:gen|????????/[????????]adj:p:v_rod:&pron:gen|????????/[????????]adj:p:v_zna:ranim:&pron:gen" + "  /[null]null ???????/[?????]noun:anim:p:v_rod:prop:lname|???????/[?????]noun:anim:p:v_zna:prop:lname|???????/[???????]adj:m:v_kly|???????/[???????]adj:m:v_naz|???????/[???????]adj:m:v_zna:rinanim"), tokenizer, sentenceTokenizer, tagger, disambiguator);
        // untouched
        TestTools.myAssert("??????? ??????????", ("/[null]SENT_START ???????/[???????]adj:m:v_kly|???????/[???????]adj:m:v_naz|???????/[???????]adj:m:v_zna:rinanim" + ("|???????/[???????]noun:anim:f:v_dav:nv:np:prop:lname|???????/[???????]noun:anim:f:v_kly:nv:np:prop:lname|???????/[???????]noun:anim:f:v_mis:nv:np:prop:lname|???????/[???????]noun:anim:f:v_naz:nv:np:prop:lname|???????/[???????]noun:anim:f:v_oru:nv:np:prop:lname|???????/[???????]noun:anim:f:v_rod:nv:np:prop:lname|???????/[???????]noun:anim:f:v_zna:nv:np:prop:lname|???????/[???????]noun:anim:m:v_naz:prop:lname:xp1|???????/[???????]noun:inanim:m:v_naz:prop:geo:xp2|???????/[???????]noun:inanim:m:v_zna:prop:geo:xp2" + // + " |???????/[???????]noun:anim:m:v_naz:prop:lname:xp1|???????/[???????]noun:inanim:m:v_naz:prop:xp2|???????/[???????]noun:inanim:m:v_zna:prop:xp2"
        "  /[null]null ??????????/[??????????]noun:inanim:m:v_naz|??????????/[??????????]noun:inanim:m:v_zna")), tokenizer, sentenceTokenizer, tagger, disambiguator);
        TestTools.myAssert("?? ??????", ("/[null]SENT_START ??/[??]prep" + "  /[null]null ??????/[??????]noun:anim:m:v_mis:prop:fname"), tokenizer, sentenceTokenizer, tagger, disambiguator);
        TestTools.myAssert("???????? ???????? ?? ?????", ("/[null]SENT_START ????????/[??????]noun:anim:p:v_rod:prop:fname|????????/[??????]noun:anim:p:v_zna:prop:fname|????????/[????????]adj:m:v_kly|????????/[????????]adj:m:v_naz|????????/[????????]adj:m:v_zna:rinanim" + ("  /[null]null ????????/[???????]noun:anim:m:v_rod:prop:lname|????????/[???????]noun:anim:m:v_zna:prop:lname" + "  /[null]null ??/[??]conj:coord|??/[??]part  /[null]null ?????/[?????]noun:anim:m:v_zna:prop:lname|?????/[?????]noun:inanim:f:v_zna")), tokenizer, sentenceTokenizer, tagger, disambiguator);
    }

    @Test
    public void testDisambiguatorForInitials() throws IOException {
        TestTools.myAssert("?.????????", ("/[null]SENT_START" + (" ?./[?.]noun:anim:f:v_naz:prop:fname:abbr|?./[?.]noun:anim:m:v_rod:prop:fname:abbr|?./[?.]noun:anim:m:v_zna:prop:fname:abbr" + " ????????/[???????]noun:anim:m:v_rod:prop:lname|????????/[???????]noun:anim:m:v_zna:prop:lname|????????/[????????]noun:anim:f:v_naz:prop:lname")), tokenizer, sentenceTokenizer, tagger, disambiguator);
        TestTools.myAssert(" ?. ????????", ("/[null]SENT_START" + ((("  /[null]null" + " ?./[?.]noun:anim:f:v_naz:prop:fname:abbr|?./[?.]noun:anim:m:v_rod:prop:fname:abbr|?./[?.]noun:anim:m:v_zna:prop:fname:abbr") + "  /[null]null") + " ????????/[???????]noun:anim:m:v_rod:prop:lname|????????/[???????]noun:anim:m:v_zna:prop:lname|????????/[????????]noun:anim:f:v_naz:prop:lname")), tokenizer, sentenceTokenizer, tagger, disambiguator);
        TestTools.myAssert(" \u0404.\u00a0\u0411\u0430\u043a\u0443\u043b\u0456\u043d\u0430", ("/[null]SENT_START" + ((("  /[null]null" + " ?./[?.]noun:anim:f:v_naz:prop:fname:abbr|?./[?.]noun:anim:m:v_rod:prop:fname:abbr|?./[?.]noun:anim:m:v_zna:prop:fname:abbr") + " \u00a0/[null]null") + " ????????/[???????]noun:anim:m:v_rod:prop:lname|????????/[???????]noun:anim:m:v_zna:prop:lname|????????/[????????]noun:anim:f:v_naz:prop:lname")), tokenizer, sentenceTokenizer, tagger, disambiguator);
        TestTools.myAssert("?.?.????????", ("/[null]SENT_START" + ((" ?./[?.]noun:anim:f:v_naz:prop:fname:abbr|?./[?.]noun:anim:m:v_rod:prop:fname:abbr|?./[?.]noun:anim:m:v_zna:prop:fname:abbr" + " ?./[?.]noun:anim:f:v_naz:prop:pname:abbr|?./[?.]noun:anim:m:v_rod:prop:pname:abbr|?./[?.]noun:anim:m:v_zna:prop:pname:abbr") + " ????????/[???????]noun:anim:m:v_rod:prop:lname|????????/[???????]noun:anim:m:v_zna:prop:lname|????????/[????????]noun:anim:f:v_naz:prop:lname")), tokenizer, sentenceTokenizer, tagger, disambiguator);
        TestTools.myAssert(" ?. ?. ????????", ("/[null]SENT_START" + ((((("  /[null]null" + " ?./[?.]noun:anim:f:v_naz:prop:fname:abbr|?./[?.]noun:anim:m:v_rod:prop:fname:abbr|?./[?.]noun:anim:m:v_zna:prop:fname:abbr") + "  /[null]null") + " ?./[?.]noun:anim:f:v_naz:prop:pname:abbr|?./[?.]noun:anim:m:v_rod:prop:pname:abbr|?./[?.]noun:anim:m:v_zna:prop:pname:abbr") + "  /[null]null") + " ????????/[???????]noun:anim:m:v_rod:prop:lname|????????/[???????]noun:anim:m:v_zna:prop:lname|????????/[????????]noun:anim:f:v_naz:prop:lname")), tokenizer, sentenceTokenizer, tagger, disambiguator);
        TestTools.myAssert("???????? ?.", ("/[null]SENT_START" + ((" ????????/[???????]noun:anim:m:v_rod:prop:lname|????????/[???????]noun:anim:m:v_zna:prop:lname|????????/[????????]noun:anim:f:v_naz:prop:lname" + "  /[null]null") + " ?./[?.]noun:anim:f:v_naz:prop:fname:abbr|?./[?.]noun:anim:m:v_rod:prop:fname:abbr|?./[?.]noun:anim:m:v_zna:prop:fname:abbr")), tokenizer, sentenceTokenizer, tagger, disambiguator);
        TestTools.myAssert(" ???????? ?. ?.", ("/[null]SENT_START" + ((((("  /[null]null" + " ????????/[???????]noun:anim:m:v_rod:prop:lname|????????/[???????]noun:anim:m:v_zna:prop:lname|????????/[????????]noun:anim:f:v_naz:prop:lname") + "  /[null]null") + " ?./[?.]noun:anim:f:v_naz:prop:fname:abbr|?./[?.]noun:anim:m:v_rod:prop:fname:abbr|?./[?.]noun:anim:m:v_zna:prop:fname:abbr") + "  /[null]null") + " ?./[?.]noun:anim:f:v_naz:prop:pname:abbr|?./[?.]noun:anim:m:v_rod:prop:pname:abbr|?./[?.]noun:anim:m:v_zna:prop:pname:abbr")), tokenizer, sentenceTokenizer, tagger, disambiguator);
        TestTools.myAssert("?.?. ????????", ("/[null]SENT_START" + (((" ?./[?.]noun:anim:f:v_naz:prop:fname:abbr|?./[?.]noun:anim:m:v_rod:prop:fname:abbr|?./[?.]noun:anim:m:v_zna:prop:fname:abbr" + " ?./[?.]noun:anim:f:v_naz:prop:pname:abbr|?./[?.]noun:anim:m:v_rod:prop:pname:abbr|?./[?.]noun:anim:m:v_zna:prop:pname:abbr") + "  /[null]null") + " ????????/[???????]noun:anim:m:v_rod:prop:lname|????????/[???????]noun:anim:m:v_zna:prop:lname|????????/[????????]noun:anim:f:v_naz:prop:lname")), tokenizer, sentenceTokenizer, tagger, disambiguator);
        TestTools.myAssert(" ?. ?. ???????? ? ?. ?. ???????", ("/[null]SENT_START" + ((((((((((((("  /[null]null" + " ?./[?.]noun:anim:f:v_naz:prop:fname:abbr|?./[?.]noun:anim:m:v_rod:prop:fname:abbr|?./[?.]noun:anim:m:v_zna:prop:fname:abbr") + "  /[null]null") + " ?./[?.]noun:anim:f:v_naz:prop:pname:abbr|?./[?.]noun:anim:m:v_rod:prop:pname:abbr|?./[?.]noun:anim:m:v_zna:prop:pname:abbr") + "  /[null]null") + " ????????/[???????]noun:anim:m:v_rod:prop:lname|????????/[???????]noun:anim:m:v_zna:prop:lname|????????/[????????]noun:anim:f:v_naz:prop:lname") + "  /[null]null") + " ?/[?]conj:coord|?/[?]part") + "  /[null]null") + " ?./[?.]noun:anim:m:v_naz:prop:fname:abbr") + "  /[null]null") + " ?./[?.]noun:anim:m:v_naz:prop:pname:abbr") + "  /[null]null") + " ???????/[???????]noun:anim:m:v_naz:prop:lname")), tokenizer, sentenceTokenizer, tagger, disambiguator);
        TestTools.myAssert("?. ???????.", ("/[null]SENT_START" + (((" ?./[?.]noun:anim:m:v_naz:prop:fname:abbr" + "  /[null]null") + " ???????/[???????]noun:anim:m:v_naz:prop:lname|???????/[???????]noun:inanim:m:v_naz:prop:geo:xp2|???????/[???????]noun:inanim:m:v_zna:prop:geo:xp2") + " ./[null]null")), tokenizer, sentenceTokenizer, tagger, disambiguator);
        TestTools.myAssert("??? ?.??????? ?.?????", ("/[null]SENT_START" + ((((((" ???/[???]prep" + "  /[null]null") + " ?./[?.]noun:anim:f:v_naz:prop:fname:abbr|?./[?.]noun:anim:m:v_rod:prop:fname:abbr|?./[?.]noun:anim:m:v_zna:prop:fname:abbr") + " ???????/[??????]noun:anim:m:v_rod:prop:lname|???????/[??????]noun:anim:m:v_zna:prop:lname|???????/[???????]noun:anim:f:v_naz:prop:lname") + "  /[null]null") + " ?./[?.]noun:anim:m:v_naz:prop:fname:abbr") + " ?????/[?????]noun:anim:m:v_naz:prop:lname")), tokenizer, sentenceTokenizer, tagger, disambiguator);
        // make sure we don't choke on complex test
        TestTools.myAssert("?????????, ??????????? ?.??. ????????, ??? ??????? ???????.", ("/[null]SENT_START ?????????/[?????????]noun:anim:m:v_naz:prop:lname|?????????/[?????????]noun:anim:m:v_naz ,/[null]null" + ((((((((("  /[null]null" + " ???????????/[???????????]adj:m:v_kly|???????????/[???????????]adj:m:v_naz|???????????/[???????????]adj:m:v_zna:rinanim") + "|???????????/[???????????]noun:anim:m:v_kly|???????????/[???????????]noun:anim:m:v_naz") + "  /[null]null") + " ?./[null]null") + " ?/[null]null") + " ?./[null]null") + "  /[null]null") + " ????????/[null]null ,/[null]null  /[null]null") + " ???/[????]verb:imperf:past:m  /[null]null ???????/[???????]adj:f:v_oru:compb  /[null]null ???????/[??????]noun:anim:f:v_oru ./[null]null")), tokenizer, sentenceTokenizer, tagger, disambiguator);
    }

    @Test
    public void testDisambiguatorRemove() throws IOException {
        TestTools.myAssert("?? ??????", ("/[null]SENT_START ??/[??]prep  /[null]null" + " ??????/[?????]noun:inanim:f:v_dav|??????/[?????]noun:inanim:f:v_mis|??????/[??????]adj:f:v_dav:compb|??????/[??????]adj:f:v_mis:compb"), tokenizer, sentenceTokenizer, tagger, disambiguator);
        TestTools.myAssert("?????", "/[null]SENT_START ?????/[?????]prep", tokenizer, sentenceTokenizer, tagger, disambiguator);
        TestTools.myAssert("?????", "/[null]SENT_START ?????/[?????]noun:anim:f:v_naz:prop:fname", tokenizer, sentenceTokenizer, tagger, disambiguator);
        TestTools.myAssert("???????", "/[null]SENT_START ???????/[???????]adj:m:v_oru:compb|???????/[???????]adj:n:v_oru:compb|???????/[???????]adj:p:v_dav:compb", tokenizer, sentenceTokenizer, tagger, disambiguator);
        TestTools.myAssert("?????", "/[null]SENT_START ?????/[?????]noun:anim:m:v_naz:prop:fname", tokenizer, sentenceTokenizer, tagger, disambiguator);
    }

    @Test
    public void testDisambiguatorForSt() throws IOException {
        TestTools.myAssert("?? ??. 208", ("/[null]SENT_START" + (((" ??/[??]prep" + "  /[null]null") + " ??./[??.]noun:inanim:f:v_dav:nv:abbr|??./[??.]noun:inanim:f:v_mis:nv:abbr|??./[??.]noun:inanim:f:v_naz:nv:abbr|??./[??.]noun:inanim:f:v_oru:nv:abbr|??./[??.]noun:inanim:f:v_rod:nv:abbr|??./[??.]noun:inanim:f:v_zna:nv:abbr") + "  /[null]null 208/[208]number")), tokenizer, sentenceTokenizer, tagger, disambiguator);
        TestTools.myAssert("?? ??. ??. 208", ("/[null]SENT_START" + (((((" ??/[??]prep" + "  /[null]null") + " ??./[??.]noun:inanim:p:v_dav:nv:abbr|??./[??.]noun:inanim:p:v_mis:nv:abbr|??./[??.]noun:inanim:p:v_naz:nv:abbr|??./[??.]noun:inanim:p:v_oru:nv:abbr|??./[??.]noun:inanim:p:v_rod:nv:abbr|??./[??.]noun:inanim:p:v_zna:nv:abbr") + "  /[null]null") + " ??./[??.]noun:inanim:p:v_dav:nv:abbr|??./[??.]noun:inanim:p:v_mis:nv:abbr|??./[??.]noun:inanim:p:v_naz:nv:abbr|??./[??.]noun:inanim:p:v_oru:nv:abbr|??./[??.]noun:inanim:p:v_rod:nv:abbr|??./[??.]noun:inanim:p:v_zna:nv:abbr") + "  /[null]null 208/[208]number")), tokenizer, sentenceTokenizer, tagger, disambiguator);
        TestTools.myAssert("? XIX ??.", ("/[null]SENT_START" + ((((" ?/[?]prep" + "  /[null]null") + " XIX/[XIX]number") + "  /[null]null") + " ??./[??.]noun:inanim:n:v_dav:nv:abbr|??./[??.]noun:inanim:n:v_mis:nv:abbr|??./[??.]noun:inanim:n:v_naz:nv:abbr|??./[??.]noun:inanim:n:v_oru:nv:abbr|??./[??.]noun:inanim:n:v_rod:nv:abbr|??./[??.]noun:inanim:n:v_zna:nv:abbr")), tokenizer, sentenceTokenizer, tagger, disambiguator);
        TestTools.myAssert("1 ??. ?????", ("/[null]SENT_START" + (((" 1/[1]number" + "  /[null]null") + " ??./[??.]adj:f:v_dav:nv:abbr|??./[??.]adj:f:v_mis:nv:abbr|??./[??.]adj:f:v_naz:nv:abbr|??./[??.]adj:f:v_oru:nv:abbr|??./[??.]adj:f:v_rod:nv:abbr|??./[??.]adj:f:v_zna:nv:abbr|??./[??.]adj:p:v_dav:nv:abbr|??./[??.]adj:p:v_mis:nv:abbr|??./[??.]adj:p:v_naz:nv:abbr|??./[??.]adj:p:v_oru:nv:abbr|??./[??.]adj:p:v_rod:nv:abbr|??./[??.]adj:p:v_zna:nv:abbr") + "  /[null]null ?????/[?????]noun:inanim:f:v_naz")), tokenizer, sentenceTokenizer, tagger, disambiguator);
        TestTools.myAssert("1 ??. ???????", ("/[null]SENT_START" + (((" 1/[1]number" + "  /[null]null") + " ??./[??.]adj:m:v_dav:nv:abbr|??./[??.]adj:m:v_mis:nv:abbr|??./[??.]adj:m:v_naz:nv:abbr|??./[??.]adj:m:v_oru:nv:abbr|??./[??.]adj:m:v_rod:nv:abbr|??./[??.]adj:m:v_zna:nv:abbr") + "  /[null]null ???????/[???????]noun:anim:m:v_naz")), tokenizer, sentenceTokenizer, tagger, disambiguator);
        TestTools.myAssert("18 ??.", ("/[null]SENT_START" + ((" 18/[18]number" + "  /[null]null") + " ??./[??.]noun:inanim:f:v_dav:nv:abbr|??./[??.]noun:inanim:f:v_mis:nv:abbr|??./[??.]noun:inanim:f:v_naz:nv:abbr|??./[??.]noun:inanim:f:v_oru:nv:abbr|??./[??.]noun:inanim:f:v_rod:nv:abbr|??./[??.]noun:inanim:f:v_zna:nv:abbr|??./[??.]noun:inanim:n:v_dav:nv:abbr|??./[??.]noun:inanim:n:v_mis:nv:abbr|??./[??.]noun:inanim:n:v_naz:nv:abbr|??./[??.]noun:inanim:n:v_oru:nv:abbr|??./[??.]noun:inanim:n:v_rod:nv:abbr|??./[??.]noun:inanim:n:v_zna:nv:abbr")), tokenizer, sentenceTokenizer, tagger, disambiguator);
    }

    @Test
    public void testTaggerUppgerGoodAndLowerBad() throws IOException {
        TestTools.myAssert("???????????????", "/[null]SENT_START ???????????????/[???????????????]noun:inanim:m:v_naz:prop|???????????????/[???????????????]noun:inanim:m:v_zna:prop", tokenizer, sentenceTokenizer, tagger, disambiguator);
    }

    @Test
    public void testTaggingForUpperCaseAbbreviations() throws IOException {
        TestTools.myAssert("???", "/[null]SENT_START ???/[???]noun:inanim:m:v_dav:nv:abbr|???/[???]noun:inanim:m:v_mis:nv:abbr|???/[???]noun:inanim:m:v_naz:nv:abbr|???/[???]noun:inanim:m:v_oru:nv:abbr|???/[???]noun:inanim:m:v_rod:nv:abbr|???/[???]noun:inanim:m:v_zna:nv:abbr|???/[???]noun:inanim:p:v_dav:nv:abbr|???/[???]noun:inanim:p:v_mis:nv:abbr|???/[???]noun:inanim:p:v_naz:nv:abbr|???/[???]noun:inanim:p:v_oru:nv:abbr|???/[???]noun:inanim:p:v_rod:nv:abbr|???/[???]noun:inanim:p:v_zna:nv:abbr", tokenizer, sentenceTokenizer, tagger, disambiguator);
        TestTools.myAssert("???", "/[null]SENT_START ???/[???]noun:inanim:f:v_dav:nv:np:abbr|???/[???]noun:inanim:f:v_mis:nv:np:abbr|???/[???]noun:inanim:f:v_naz:nv:np:abbr|???/[???]noun:inanim:f:v_oru:nv:np:abbr|???/[???]noun:inanim:f:v_rod:nv:np:abbr|???/[???]noun:inanim:f:v_zna:nv:np:abbr", tokenizer, sentenceTokenizer, tagger, disambiguator);
    }

    @Test
    public void testSimpleRemove() throws IOException {
        TestTools.myAssert("????", "/[null]SENT_START ????/[????]verb:imperf:past:f", tokenizer, sentenceTokenizer, tagger, disambiguator);
        TestTools.myAssert("????-??", "/[null]SENT_START ????-??/[????]verb:imperf:past:f", tokenizer, sentenceTokenizer, tagger, disambiguator);
    }

    @Test
    public void testDisambiguatorRemovePresentInDictionary() throws IOException {
        // make sure our disambiguation lines are valid lines in dictionary
        Map<String, TokenMatcher> map = new SimpleDisambiguator().DISAMBIG_REMOVE_MAP;
        for (Map.Entry<String, TokenMatcher> entry : map.entrySet()) {
            List<AnalyzedTokenReadings> tagged = tagger.tag(Arrays.asList(entry.getKey()));
            AnalyzedTokenReadings taggedToken = tagged.get(0);
            TokenMatcher tokenMatcher = entry.getValue();
            Assert.assertTrue(String.format("%s not found in dictionary, tags: %s", entry.toString(), tagged.toString()), UkrainianDisambiguationRuleTest.matches(taggedToken, tokenMatcher));
        }
    }

    @Test
    public void testChunker() throws Exception {
        JLanguageTool lt = new JLanguageTool(new Ukrainian());
        AnalyzedSentence analyzedSentence = lt.getAnalyzedSentence("???  ????????.");
        AnalyzedSentence disambiguated = chunker.disambiguate(analyzedSentence);
        AnalyzedTokenReadings[] tokens = disambiguated.getTokens();
        Assert.assertTrue(tokens[1].getReadings().toString().contains("<adv>"));
        Assert.assertTrue(tokens[4].getReadings().toString().contains("<adv>"));
        analyzedSentence = lt.getAnalyzedSentence("?? ???? ?????");
        disambiguated = chunker.disambiguate(analyzedSentence);
        tokens = disambiguated.getTokens();
        Assert.assertTrue(tokens[1].getReadings().toString().contains("<insert>"));
        Assert.assertTrue(tokens[3].getReadings().toString().contains("<insert>"));
        Assert.assertTrue(tokens[5].getReadings().toString().contains("<insert>"));
    }
}

